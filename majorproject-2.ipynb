{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Environment Setup for Kaggle\n# The '!' tells Kaggle to run this as a terminal command\n!pip install -qU langgraph langchain langchain_community langchain_huggingface\n!pip install -qU bitsandbytes accelerate transformers\n!pip install -qU tiktoken faiss-cpu\n\nprint(\"Libraries installed successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:18:39.690216Z","iopub.execute_input":"2025-12-02T04:18:39.690916Z","iopub.status.idle":"2025-12-02T04:20:19.036340Z","shell.execute_reply.started":"2025-12-02T04:18:39.690888Z","shell.execute_reply":"2025-12-02T04:20:19.035361Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hLibraries installed successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# # Option A: Interactive Login (Easiest - just paste token when asked)\n# login()\n\n# Option B: Automatic (If you don't want to paste every time)\n# 1. In Kaggle menu, go to \"Add-ons\" -> \"Secrets\"\n# 2. Add a new secret called \"HF_TOKEN\" and paste your token there.\n# 3. Uncomment the lines below:\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:20:38.696818Z","iopub.execute_input":"2025-12-02T04:20:38.697105Z","iopub.status.idle":"2025-12-02T04:20:39.495158Z","shell.execute_reply.started":"2025-12-02T04:20:38.697078Z","shell.execute_reply":"2025-12-02T04:20:39.494377Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nfrom langchain_huggingface import HuggingFacePipeline\n\n# 1. Configuration for 4-Bit Quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# 2. Load Model\n# Make sure you have accepted the license on the HuggingFace Llama-3 page!\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nprint(\"Loading Model... (This takes about 2-3 mins on Kaggle)\")\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n    \n    # We load the model onto the first GPU\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        quantization_config=quantization_config,\n        device_map=\"auto\"\n    )\n\n    # 3. Create the Pipeline\n    text_generation_pipeline = HuggingFacePipeline(pipeline=pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=1024,\n        temperature=0.1,\n        return_full_text=False\n    ))\n    \n    print(\"SUCCESS: Model Loaded on GPU.\")\n\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    print(\"Tip: Did you turn on the GPU in the sidebar?\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:31:02.815854Z","iopub.execute_input":"2025-12-02T04:31:02.816542Z","iopub.status.idle":"2025-12-02T04:32:39.520310Z","shell.execute_reply.started":"2025-12-02T04:31:02.816516Z","shell.execute_reply":"2025-12-02T04:32:39.519552Z"}},"outputs":[{"name":"stdout","text":"Loading Model... (This takes about 2-3 mins on Kaggle)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fca48697287f44f295b9cdc7de2e714f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e5a6493f0b4b8790284c97a4b2b223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0760808e9c2e41ae9516ce1bf68a8019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f08e2052b8e49dbb9ffc39974bb0223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6f643588384943bcb259ec4b43c1eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"083204bf8353497482a90afdcf089408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d51a373569a4699915f9d1b62002877"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499f0047798a465faecb0f2b8e3c07b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef9310279484298abe80ab6dc96f0f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437852bbf06f4ad8a5703dd9aa537ddb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19c8d4517b494171a873a9dcae6532df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9c0c63c9d2466aaa79aa572f840987"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"SUCCESS: Model Loaded on GPU.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\n\n# --- 1. The Analyst's Persona (System Prompt) ---\n# We force Llama-3 to act as a strict critic.\nanalyst_template = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are an Expert Prompt Analyst. Your job is to critique user queries for Large Language Models.\nYou do NOT rewrite the prompt. You only identify weaknesses.\n\nAnalyze the user's input for:\n1. Ambiguity (Vague intent)\n2. Missing Context (Target audience, format, specific technologies)\n3. Constructive Feedback (What specific details need to be added?)\n\nOutput your analysis in this exact JSON format:\n{{\n    \"is_ambiguous\": \"yes/no\",\n    \"missing_context\": [\"list\", \"of\", \"missing\", \"items\"],\n    \"critique\": \"A brief summary of what is wrong.\"\n}}\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nUser Input: {user_input}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nanalyst_prompt = PromptTemplate(\n    template=analyst_template,\n    input_variables=[\"user_input\"]\n)\n\n# --- 2. The Node Function ---\ndef analyst_node(state):\n    \"\"\"\n    Node 1: Analyzes the raw input and updates the state with a critique.\n    \"\"\"\n    print(f\"\\n--- [Node 1] Analyst is thinking... ---\")\n    \n    # 1. Get user input from the state\n    user_query = state[\"user_input\"]\n    \n    # 2. Invoke the LLM\n    # We chain the prompt -> model -> string parser\n    chain = analyst_prompt | text_generation_pipeline\n    \n    response = chain.invoke({\"user_input\": user_query})\n    \n    # 3. Simple parsing (Handling potential extra text from Llama-3)\n    # Sometimes models chatter before the JSON. We find the JSON structure.\n    import json\n    import re\n    \n    try:\n        # Extract JSON using regex (robustness for M.Tech projects)\n        json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n        if json_match:\n            critique_data = json.loads(json_match.group())\n        else:\n            # Fallback if model fails JSON format\n            critique_data = {\"error\": \"Failed to parse JSON\", \"raw\": response}\n            \n    except Exception as e:\n        critique_data = {\"error\": str(e)}\n\n    # 4. Update the state\n    # We save the critique so the next agent (Architect) can see it.\n    state[\"critique\"] = critique_data\n    \n    print(f\"Critique Generated: {critique_data['critique']}\")\n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:33:25.380734Z","iopub.execute_input":"2025-12-02T04:33:25.381024Z","iopub.status.idle":"2025-12-02T04:33:25.407195Z","shell.execute_reply.started":"2025-12-02T04:33:25.381002Z","shell.execute_reply":"2025-12-02T04:33:25.406355Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"not need every time to run below cell\n","metadata":{}},{"cell_type":"code","source":"# --- TEST ZONE ---\n# Let's test with a purposefully bad prompt\ntest_state = {\n    \"user_input\": \"write a code for snake game\",  # Vague: No language, no library specified\n    \"retry_count\": 0,\n    \"critique\": None,\n    \"refined_prompt\": \"\",\n    \"score\": 0,\n    \"feedback_logs\": []\n}\n\n# Run the node\nresult_state = analyst_node(test_state)\n\n# Display the result\nprint(\"\\n--- TEST RESULTS ---\")\nprint(f\"Input: {result_state['user_input']}\")\nprint(f\"Analyst Findings: {result_state['critique']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:33:38.596854Z","iopub.execute_input":"2025-12-02T04:33:38.597146Z","iopub.status.idle":"2025-12-02T04:33:50.351563Z","shell.execute_reply.started":"2025-12-02T04:33:38.597111Z","shell.execute_reply":"2025-12-02T04:33:50.350916Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 1] Analyst is thinking... ---\nCritique Generated: The prompt is too vague. It's unclear what kind of code the user wants to write (e.g., console-based, graphical, mobile app), what features they want to include (e.g., scoring, levels, multiplayer), and what programming language they want to use. Providing more context would help the model generate a more accurate and relevant response.\n\n--- TEST RESULTS ---\nInput: write a code for snake game\nAnalyst Findings: {'is_ambiguous': 'yes', 'missing_context': ['specific programming language', 'desired features', 'target platform'], 'critique': \"The prompt is too vague. It's unclear what kind of code the user wants to write (e.g., console-based, graphical, mobile app), what features they want to include (e.g., scoring, levels, multiplayer), and what programming language they want to use. Providing more context would help the model generate a more accurate and relevant response.\"}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import re\nfrom langchain_core.prompts import PromptTemplate\n\n# --- 1. The Architect's Persona (Updated for Flexibility) ---\narchitect_template = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are the \"Architect,\" an advanced prompt engineering agent.\nYour goal is to rewrite the user's raw prompt into a high-fidelity instruction using the CO-STAR framework.\n\nINPUT DATA:\n1. User Raw Input: {user_input}\n2. Analyst's Critique: {critique}\n\nINSTRUCTIONS:\n- Fix the issues identified in the critique.\n- Use the CO-STAR format for the rewrite:\n  (C)ontext: Provide background.\n  (O)bjective: Define the specific task.\n  (S)tyle: Define the style (e.g., PEP-8 for code, Professional for text).\n  (T)one: Professional and technical.\n  (A)udience: Define the audience (e.g., Expert Developer or Hiring Manager).\n  (R)esponse: Define the format (e.g., Single Code Block, Bullet Points).\n\nIMPORTANT:\n- If the user asks for CODE, assume Python unless specified otherwise.\n- If the user asks for TEXT (Essay, Interview, Email), do NOT mention code standards like PEP-8.\n- OUTPUT ONLY THE REWRITTEN PROMPT. Do not say \"Here is the prompt\".\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nRefine this prompt.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\narchitect_prompt = PromptTemplate(\n    template=architect_template,\n    input_variables=[\"user_input\", \"critique\"]\n)\n\n# --- 2. The Node Function (With Auto-Cleaning) ---\ndef architect_node(state):\n    \"\"\"\n    Node 2: Rewrites the prompt based on the critique.\n    Includes Regex cleaning to prevent 'blank output' errors.\n    \"\"\"\n    print(f\"\\n--- [Node 2] Architect is designing... (Attempt {state.get('retry_count', 0) + 1}) ---\")\n    \n    # 1. Get data\n    user_query = state[\"user_input\"]\n    critique_data = state[\"critique\"]\n    \n    # 2. Invoke LLM\n    chain = architect_prompt | text_generation_pipeline\n    raw_output = chain.invoke({\n        \"user_input\": user_query,\n        \"critique\": str(critique_data)\n    })\n    \n    # --- 3. CLEAN THE OUTPUT (The Fix) ---\n    # We strip out conversational filler to ensure Llama-3 answers the prompt later.\n    \n    # Pattern A: Look for text inside double quotes at the end (Common Llama-3 habit)\n    match_quotes = re.search(r'\"([^\"]*)\"\\s*$', raw_output, re.DOTALL)\n    \n    # Pattern B: Look for text after common intro phrases\n    match_text = re.search(r\"(?:Here is the rewritten prompt:|Rewritten Prompt:)\\s*(.*)\", raw_output, re.DOTALL | re.IGNORECASE)\n    \n    if match_quotes:\n        cleaned_prompt = match_quotes.group(1)\n        print(\">> DEBUG: Extracted prompt from quotes.\")\n    elif match_text:\n        cleaned_prompt = match_text.group(1)\n        print(\">> DEBUG: Extracted prompt after filler text.\")\n    else:\n        # Fallback: Use the whole thing if no filler detected\n        cleaned_prompt = raw_output\n\n    # Clean whitespace\n    cleaned_prompt = cleaned_prompt.strip()\n    \n    # 4. Update State\n    state[\"refined_prompt\"] = cleaned_prompt\n    \n    # Increment Retry Counter\n    state[\"retry_count\"] = state.get(\"retry_count\", 0) + 1\n    \n    print(f\"Prompt Rewritten (Length: {len(cleaned_prompt)} chars).\")\n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T05:11:50.982851Z","iopub.execute_input":"2025-12-02T05:11:50.983578Z","iopub.status.idle":"2025-12-02T05:11:50.990087Z","shell.execute_reply.started":"2025-12-02T05:11:50.983553Z","shell.execute_reply":"2025-12-02T05:11:50.989407Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"below cell not need to rum everytime","metadata":{}},{"cell_type":"code","source":"# --- TEST ZONE ---\n# We continue from the previous 'result_state'\n# It already has 'user_input' and 'critique' inside.\n\nfinal_state = architect_node(result_state)\n\nprint(\"\\n--- ARCHITECT OUTPUT ---\")\nprint(final_state[\"refined_prompt\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:41:28.439839Z","iopub.execute_input":"2025-12-02T04:41:28.440384Z","iopub.status.idle":"2025-12-02T04:41:40.283057Z","shell.execute_reply.started":"2025-12-02T04:41:28.440359Z","shell.execute_reply":"2025-12-02T04:41:40.282425Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 2] Architect is designing... (Attempt 1) ---\nPrompt Rewritten.\n\n--- ARCHITECT OUTPUT ---\nHere is the rewritten prompt in the CO-STAR format:\n\n**(C)ontext:** The goal is to create a console-based Snake game using Python.\n\n**(O)bjective:** Write a Python code that implements a basic Snake game, featuring a snake that moves around a grid, eats food pellets, and grows in length.\n\n**(S)tyle:** Clean, following PEP-8 guidelines.\n\n**(T)one:** Professional and technical.\n\n**(A)udience:** Expert Python developer.\n\n**(R)esponse:** Single Code Block\n\nPlease provide the rewritten prompt.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# --- 1. The Judge's Persona (G-Eval Logic) ---\njudge_template = \"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are the \"Supreme Judge\" of prompt quality.\nEvaluate the \"Refined Prompt\" based on a strict rubric:\n1. Clarity (Is the objective clear?)\n2. Specificity (Are languages/tools defined?)\n3. Robustness (Does it handle constraints?)\n\nGive a score from 0 to 10 (10 is perfect).\nIf the score is below 8, provide specific feedback on what to fix.\n\nFormat your response as valid JSON:\n{{\n    \"score\": 8,\n    \"feedback\": \"Excellent prompt, no changes needed.\"\n}}\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nRefined Prompt: {refined_prompt}\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\njudge_prompt = PromptTemplate(\n    template=judge_template,\n    input_variables=[\"refined_prompt\"]\n)\n\n# --- 2. The Node Function ---\ndef judge_node(state):\n    \"\"\"\n    Node 3: Scores the prompt.\n    \"\"\"\n    print(f\"\\n--- [Node 3] Judge is evaluating... ---\")\n    \n    refined_prompt = state[\"refined_prompt\"]\n    \n    # Invoke\n    chain = judge_prompt | text_generation_pipeline\n    response = chain.invoke({\"refined_prompt\": refined_prompt})\n    \n    # Parse JSON\n    import json\n    import re\n    \n    try:\n        json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n        if json_match:\n            judge_data = json.loads(json_match.group())\n            score = int(judge_data.get(\"score\", 0))\n            feedback = judge_data.get(\"feedback\", \"No feedback.\")\n        else:\n            score = 5 # Penalty for bad formatting\n            feedback = \"Judge could not parse output.\"\n            \n    except Exception as e:\n        score = 0\n        feedback = str(e)\n\n    # Update State\n    state[\"score\"] = score\n    state[\"feedback_logs\"].append(f\"Score: {score} | Feedback: {feedback}\")\n    \n    print(f\"Verdict: Score {score}/10\")\n    print(f\"Feedback: {feedback}\")\n    \n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T05:12:01.594830Z","iopub.execute_input":"2025-12-02T05:12:01.595096Z","iopub.status.idle":"2025-12-02T05:12:01.601779Z","shell.execute_reply.started":"2025-12-02T05:12:01.595077Z","shell.execute_reply":"2025-12-02T05:12:01.601016Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# --- The Router Function ---\ndef decide_next_step(state):\n    \"\"\"\n    Determines the path:\n    - If Score > 8: End\n    - If Score < 8 AND Retry Count < 3: Retry (Back to Architect)\n    - Else: End (Give up to prevent infinite loops)\n    \"\"\"\n    score = state[\"score\"]\n    retry_count = state[\"retry_count\"]\n    \n    if score >= 8:\n        print(\">>> Decision: APPROVED. Proceeding to Output.\")\n        return \"end\"\n    \n    if retry_count < 3:\n        print(f\">>> Decision: REJECTED (Score {score}). Retrying... (Attempt {retry_count+1}/3)\")\n        return \"retry\"\n    \n    print(\">>> Decision: MAX RETRIES REACHED. Proceeding anyway.\")\n    return \"end\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T05:12:08.440825Z","iopub.execute_input":"2025-12-02T05:12:08.441093Z","iopub.status.idle":"2025-12-02T05:12:08.445580Z","shell.execute_reply.started":"2025-12-02T05:12:08.441071Z","shell.execute_reply":"2025-12-02T05:12:08.444943Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from typing import TypedDict, List, Optional\nfrom langgraph.graph import StateGraph, END\n\n# --- 1. RE-DEFINE THE STATE (The Memory) ---\nclass AgentState(TypedDict):\n    user_input: str             # The raw prompt from the user\n    target_model: str           # e.g., \"Llama-3\"\n    \n    critique: Optional[dict]    # Analyst output\n    refined_prompt: str         # Architect output\n    score: int                  # Judge score\n    feedback_logs: List[str]    # History\n    \n    retry_count: int            # Loop limiter\n\n# --- 2. RE-DEFINE THE ROUTER (The Decision Logic) ---\ndef decide_next_step(state):\n    \"\"\"\n    Determines if we loop back or finish.\n    \"\"\"\n    score = state.get(\"score\", 0)\n    retry_count = state.get(\"retry_count\", 0)\n    \n    # If score is good (8+), we stop.\n    if score >= 8:\n        print(\">>> Decision: APPROVED. Proceeding to Output.\")\n        return \"end\"\n    \n    # If score is bad, but we have retries left, we loop.\n    if retry_count < 3:\n        # Increment retry count manually here if needed, \n        # or handle it in the Architect node (we'll update state below)\n        print(f\">>> Decision: REJECTED (Score {score}). Retrying... (Attempt {retry_count+1}/3)\")\n        return \"retry\"\n    \n    # If out of retries, we force stop.\n    print(\">>> Decision: MAX RETRIES REACHED. Proceeding anyway.\")\n    return \"end\"\n\n# --- 3. BUILD THE GRAPH ---\nworkflow = StateGraph(AgentState)\n\n# Add Nodes (Make sure analyst_node, architect_node, judge_node are defined above!)\nworkflow.add_node(\"analyst\", analyst_node)\nworkflow.add_node(\"architect\", architect_node)\nworkflow.add_node(\"judge\", judge_node)\n\n# Set the Entry Point\nworkflow.set_entry_point(\"analyst\")\n\n# Connect the Nodes (Normal flow)\nworkflow.add_edge(\"analyst\", \"architect\")\nworkflow.add_edge(\"architect\", \"judge\")\n\n# Connect the Conditional Edge (The Loop)\nworkflow.add_conditional_edges(\n    \"judge\",\n    decide_next_step,\n    {\n        \"retry\": \"architect\", \n        \"end\": END\n    }\n)\n\n# Compile\napp = workflow.compile()\nprint(\"Graph Compiled Successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T05:12:15.739707Z","iopub.execute_input":"2025-12-02T05:12:15.739971Z","iopub.status.idle":"2025-12-02T05:12:15.752611Z","shell.execute_reply.started":"2025-12-02T05:12:15.739953Z","shell.execute_reply":"2025-12-02T05:12:15.751843Z"}},"outputs":[{"name":"stdout","text":"Graph Compiled Successfully!\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# --- FINAL EXECUTION ---\ninitial_input = {\n    \"user_input\": \"make a snake game code\",\n    \"retry_count\": 0,\n    \"feedback_logs\": []\n}\n\nfinal_output = app.invoke(initial_input, config={\"recursion_limit\": 10})\n\nprint(\"\\n======================================\")\nprint(\"FINAL OPTIMIZED PROMPT:\")\nprint(\"======================================\")\nprint(final_output[\"refined_prompt\"])\nprint(\"\\nFinal Score:\", final_output[\"score\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:42:10.747702Z","iopub.execute_input":"2025-12-02T04:42:10.748302Z","iopub.status.idle":"2025-12-02T04:42:49.100639Z","shell.execute_reply.started":"2025-12-02T04:42:10.748275Z","shell.execute_reply":"2025-12-02T04:42:49.099882Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 1] Analyst is thinking... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Critique Generated: The prompt is too vague. It's unclear what kind of snake game is being requested (e.g., console-based, graphical, 2D, 3D), what features are required (e.g., scoring, levels, multiplayer), and what programming language or platform is preferred. Providing more specific details would help generate a more accurate and relevant response.\n\n--- [Node 2] Architect is designing... (Attempt 1) ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt Rewritten.\n\n--- [Node 3] Judge is evaluating... ---\nVerdict: Score 9/10\nFeedback: Excellent prompt! The CO-STAR format is well-structured, and the requirements are clear and concise. The only minor suggestion is to consider adding a specific constraint on the game's difficulty level or a target score for the game. This would make the prompt more robust and challenging for the developer.\n>>> Decision: APPROVED. Proceeding to Output.\n\n======================================\nFINAL OPTIMIZED PROMPT:\n======================================\nHere is the rewritten prompt in the CO-STAR format:\n\n**(C)ontext:** A console-based, 2D snake game is to be developed using Python.\n\n**(O)bjective:** Write a Python code that implements a console-based, 2D snake game with the following features: scoring, levels, and basic game logic.\n\n**(S)tyle:** PEP-8 compliant code with proper indentation and comments.\n\n**(T)one:** Professional and technical.\n\n**(A)udience:** Expert Python developer.\n\n**(R)esponse:** A single code block in Markdown format.\n\nHere is the rewritten prompt:\n```\nWrite a Python code that implements a console-based, 2D snake game with scoring, levels, and basic game logic. The game should be playable in a terminal or command prompt. The code should be written in PEP-8 style and include proper indentation and comments. The output should be a single code block in Markdown format.\n```\n\nFinal Score: 9\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import pandas as pd\nimport time\n\n# --- 1. The Test Dataset (3 Distinct Categories) ---\ntest_prompts = [\n    # 1. Coding (Ambiguous)\n    \"write a python script for scraping data\",\n    \n    # 2. Reasoning (Missing Context)\n    \"explain quantum physics\",\n    \n    # 3. Creative/Business (Tone Mismatch)\n    \"write an email to my boss asking for leave\"\n]\n\nresults = []\n\nprint(f\"--- STARTING TWIN-TEST BENCHMARK (n={len(test_prompts)}) ---\\n\")\n\nfor i, raw_prompt in enumerate(test_prompts):\n    print(f\"Processing Prompt {i+1}: '{raw_prompt}'...\")\n    \n    # --- PATH A: THE \"DUMB\" MODEL (Control) ---\n    start_time = time.time()\n    \n    # FIX: Use .invoke() instead of calling it directly\n    response_a = text_generation_pipeline.invoke(raw_prompt)\n    \n    time_a = time.time() - start_time\n    \n    # --- PATH B: THE \"APE\" SYSTEM (Experiment) ---\n    start_time = time.time()\n    \n    # 1. Optimize the prompt\n    initial_input = {\"user_input\": raw_prompt, \"retry_count\": 0, \"feedback_logs\": []}\n    ape_result = app.invoke(initial_input, config={\"recursion_limit\": 10})\n    optimized_prompt = ape_result[\"refined_prompt\"]\n    score = ape_result[\"score\"]\n    \n    # 2. Generate final answer using the optimized prompt\n    # FIX: Use .invoke() here as well\n    response_b = text_generation_pipeline.invoke(optimized_prompt)\n    \n    time_b = time.time() - start_time\n    \n    # --- SAVE RESULTS ---\n    # We truncate strings to 100 chars to keep the table clean\n    results.append({\n        \"Original Prompt\": raw_prompt,\n        \"Optimized Prompt\": optimized_prompt[:100] + \"...\", \n        \"Judge Score\": score,\n        \"Response A (Raw)\": response_a[:100] + \"...\",      \n        \"Response B (APE)\": response_b[:100] + \"...\"       \n    })\n\nprint(\"\\n--- BENCHMARK COMPLETE ---\")\n\n# --- DISPLAY AS TABLE ---\ndf = pd.DataFrame(results)\npd.set_option('display.max_colwidth', None) # Show full text if needed\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:50:04.057505Z","iopub.execute_input":"2025-12-02T04:50:04.057766Z","iopub.status.idle":"2025-12-02T04:56:29.212036Z","shell.execute_reply.started":"2025-12-02T04:50:04.057747Z","shell.execute_reply":"2025-12-02T04:56:29.211227Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"--- STARTING TWIN-TEST BENCHMARK (n=3) ---\n\nProcessing Prompt 1: 'write a python script for scraping data'...\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 1] Analyst is thinking... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Critique Generated: The prompt is too vague and lacks essential details. It's unclear what data the user wants to scrape, from where, and in what format. Providing more context about the data source, target data, and desired output format would help the model generate a more accurate and relevant script.\n\n--- [Node 2] Architect is designing... (Attempt 1) ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt Rewritten.\n\n--- [Node 3] Judge is evaluating... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Verdict: Score 9/10\nFeedback: Excellent prompt! The CO-STAR format is well-structured, and the objective is clear. The only suggestion is to specify the target data, specific data source, and desired output format in the prompt itself, rather than leaving them as placeholders. This will help the developer understand the requirements better and provide a more accurate response.\n>>> Decision: APPROVED. Proceeding to Output.\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing Prompt 2: 'explain quantum physics'...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 1] Analyst is thinking... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Critique Generated: The prompt is too broad and open-ended, making it difficult to provide a concise and meaningful response. Without specifying a particular topic or concept within quantum physics, it's hard to know where to start. Additionally, the lack of context about the target audience and desired format makes it challenging to tailor the response to the user's needs.\n\n--- [Node 2] Architect is designing... (Attempt 1) ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt Rewritten.\n\n--- [Node 3] Judge is evaluating... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Verdict: Score 9/10\nFeedback: Excellent prompt! The CO-STAR format is well-structured, and the objective is clear. The tone and style are well-defined, and the audience is accurately targeted. The only minor suggestion is to consider adding a specific constraint on the length of the response, as it's not explicitly mentioned. Overall, this prompt is well-crafted and ready for use.\n>>> Decision: APPROVED. Proceeding to Output.\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Processing Prompt 3: 'write an email to my boss asking for leave'...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 1] Analyst is thinking... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Critique Generated: The prompt is too vague and lacks specific details. It's unclear who the target audience is (e.g., specific boss, HR department), what type of leave is being requested (e.g., vacation, sick leave), and what format the email should take (e.g., formal, informal). Providing more context and details would help the model generate a more effective email.\n\n--- [Node 2] Architect is designing... (Attempt 1) ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt Rewritten.\n\n--- [Node 3] Judge is evaluating... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Verdict: Score 9/10\nFeedback: Excellent prompt! The CO-STAR format is well-structured, and the objective is clear. However, there's room for improvement in the 'Style' and 'Response' sections. Consider adding more specific guidelines for the tone and response format to make it more robust.\n>>> Decision: APPROVED. Proceeding to Output.\n\n--- BENCHMARK COMPLETE ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                              Original Prompt  \\\n0     write a python script for scraping data   \n1                     explain quantum physics   \n2  write an email to my boss asking for leave   \n\n                                                                                            Optimized Prompt  \\\n0  Here is the rewritten prompt in the CO-STAR format:\\n\\n**(C)ontext:** We aim to develop a Python scrip...   \n1  Here is the rewritten prompt in the CO-STAR format:\\n\\n(C)ontext: Quantum physics is a fundamental bra...   \n2  Here is the rewritten prompt in the CO-STAR format:\\n\\n(C)ontext: I am an employee seeking to request ...   \n\n   Judge Score  \\\n0            9   \n1            9   \n2            9   \n\n                                                                                                Response A (Raw)  \\\n0   from a website\\n```\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Send a GET request\\nresponse = r...   \n1        in simple terms\\nQuantum physics is a branch of physics that deals with the behavior of matter and e...   \n2    \\nHere is an example email you could send to your boss:\\nSubject: Request for Leave on [Date]\\n\\nDear [B...   \n\n                                                                                            Response B (APE)  \n0                                                                                                        ...  \n1     Use a code block to illustrate the concept, and provide a brief example of how superposition is use...  \n2   The email should be written in a professional tone and include the following information:\\n\\n* A clea...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Original Prompt</th>\n      <th>Optimized Prompt</th>\n      <th>Judge Score</th>\n      <th>Response A (Raw)</th>\n      <th>Response B (APE)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>write a python script for scraping data</td>\n      <td>Here is the rewritten prompt in the CO-STAR format:\\n\\n**(C)ontext:** We aim to develop a Python scrip...</td>\n      <td>9</td>\n      <td>from a website\\n```\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\n# Send a GET request\\nresponse = r...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>explain quantum physics</td>\n      <td>Here is the rewritten prompt in the CO-STAR format:\\n\\n(C)ontext: Quantum physics is a fundamental bra...</td>\n      <td>9</td>\n      <td>in simple terms\\nQuantum physics is a branch of physics that deals with the behavior of matter and e...</td>\n      <td>Use a code block to illustrate the concept, and provide a brief example of how superposition is use...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>write an email to my boss asking for leave</td>\n      <td>Here is the rewritten prompt in the CO-STAR format:\\n\\n(C)ontext: I am an employee seeking to request ...</td>\n      <td>9</td>\n      <td>\\nHere is an example email you could send to your boss:\\nSubject: Request for Leave on [Date]\\n\\nDear [B...</td>\n      <td>The email should be written in a professional tone and include the following information:\\n\\n* A clea...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"!pip install -q gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:57:53.528903Z","iopub.execute_input":"2025-12-02T04:57:53.529387Z","iopub.status.idle":"2025-12-02T04:57:59.950197Z","shell.execute_reply.started":"2025-12-02T04:57:53.529362Z","shell.execute_reply":"2025-12-02T04:57:59.949510Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import gradio as gr\n\n# --- 1. The Wrapper Function ---\n# This connects the UI inputs to your LangGraph logic\ndef process_prompt(user_input):\n    status_msg = \"Status: Initializing Agents...\"\n    \n    # A. Run the APE System (Analyst -> Architect -> Judge)\n    inputs = {\"user_input\": user_input, \"retry_count\": 0, \"feedback_logs\": []}\n    \n    # Run the graph\n    result = app.invoke(inputs, config={\"recursion_limit\": 10})\n    \n    refined_prompt = result[\"refined_prompt\"]\n    score = result[\"score\"]\n    critique = result[\"critique\"]\n    logs = \"\\n\".join(result[\"feedback_logs\"])\n    \n    # B. Generate Actual Responses (The \"Twin-Test\")\n    # 1. Raw Response (Control)\n    raw_response = text_generation_pipeline.invoke(user_input)\n    \n    # 2. Optimized Response (Experiment)\n    optimized_response = text_generation_pipeline.invoke(refined_prompt)\n    \n    # Format the critique for display\n    critique_text = f\"Ambiguity: {critique.get('is_ambiguous', 'N/A')}\\nMissing: {critique.get('missing_context', [])}\\n\\nAnalyst Notes: {critique.get('critique', '')}\"\n    \n    return (\n        raw_response,           # Output 1: Raw Result\n        refined_prompt,         # Output 2: The New Prompt\n        f\"Score: {score}/10\",   # Output 3: Judge Score\n        critique_text,          # Output 4: Critique\n        optimized_response      # Output 5: Final Optimized Result\n    )\n\n# --- 2. The Dashboard Layout ---\ncustom_css = \"\"\"\n#heading {text-align: center; color: #2d3748;}\n#score {font-size: 24px; font-weight: bold; color: #38a169;}\n\"\"\"\n\nwith gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n    \n    gr.Markdown(\"# ü¶ç APE: Autonomous Prompt Engineer\", elem_id=\"heading\")\n    gr.Markdown(\"### An Agentic Framework for Iterative Prompt Optimization\")\n    \n    with gr.Row():\n        input_box = gr.Textbox(\n            label=\"Enter Raw Prompt\", \n            placeholder=\"e.g., write a code for snake game...\",\n            lines=2\n        )\n        submit_btn = gr.Button(\"üöÄ Optimize & Run\", variant=\"primary\")\n\n    # --- RESULTS SECTION ---\n    with gr.Row():\n        # LEFT COLUMN: THE BEFORE\n        with gr.Column(variant=\"panel\"):\n            gr.Markdown(\"### üî¥ Control (Raw Input)\")\n            out_raw_res = gr.Textbox(label=\"Llama-3 Response (Raw)\", lines=10, interactive=False)\n\n        # RIGHT COLUMN: THE AFTER\n        with gr.Column(variant=\"panel\"):\n            gr.Markdown(\"### üü¢ Experimental (APE Agent)\")\n            \n            # Internal Agent Details (Accordion to keep it clean)\n            with gr.Accordion(\"See Agent Thinking Process\", open=False):\n                with gr.Row():\n                    out_critique = gr.Textbox(label=\"Agent 1: Analyst Critique\", lines=4)\n                    out_score = gr.Textbox(label=\"Agent 3: Judge Score\", lines=1, elem_id=\"score\")\n                out_refined = gr.Textbox(label=\"Agent 2: Architect's Rewritten Prompt\", lines=4)\n            \n            out_opt_res = gr.Textbox(label=\"Llama-3 Response (Optimized)\", lines=10, interactive=False)\n\n    # --- CLICK EVENT ---\n    submit_btn.click(\n        fn=process_prompt,\n        inputs=[input_box],\n        outputs=[out_raw_res, out_refined, out_score, out_critique, out_opt_res]\n    )\n\n# --- 3. Launch ---\n# share=True creates a public link (expires in 72h)\ndemo.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T05:12:37.833985Z","iopub.execute_input":"2025-12-02T05:12:37.834262Z","iopub.status.idle":"2025-12-02T05:34:20.705790Z","shell.execute_reply.started":"2025-12-02T05:12:37.834239Z","shell.execute_reply":"2025-12-02T05:34:20.705058Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://6d2db1371f9279030b.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://6d2db1371f9279030b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- [Node 1] Analyst is thinking... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Critique Generated: The prompt is too vague and open-ended. It's unclear what type of game the user wants to build (e.g. a video game, a board game, a card game, etc.), what aspect of Indian culture they want to focus on (e.g. mythology, festivals, daily life, etc.), and who the target audience is (e.g. children, adults, educational, entertainment, etc.). Providing more specific details would help the model better understand the user's intent and provide a more accurate response.\n\n--- [Node 2] Architect is designing... (Attempt 1) ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Prompt Rewritten (Length: 629 chars).\n\n--- [Node 3] Judge is evaluating... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Verdict: Score 9/10\nFeedback: Excellent prompt! The objective is clear, and the specificity is high. The tone and style are professional and technical, and the audience is well-defined. The only suggestion I have is to consider adding more context about the specific aspect of Indian culture to focus on, as it is currently left open-ended. For example, you could specify a particular region, era, or theme to make the prompt more concrete.\n>>> Decision: APPROVED. Proceeding to Output.\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://6d2db1371f9279030b.gradio.live\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}